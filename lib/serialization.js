// Generated by CoffeeScript 1.8.0
var MAX_SAFE_INTEGER, UINT64, crypto, fs, mkdirp, openSync, path, queue, readHeader, readHeaderOld, readUINT64, saveArchive, writeFooter, writeHeader, writeUINT64;

fs = require('fs');

path = require('path');

crypto = require('crypto');

UINT64 = require('cuint').UINT64;

mkdirp = require('mkdirp');

queue = require('queue-async');

MAX_SAFE_INTEGER = 9007199254740992;

writeUINT64 = function(buf, val, ofs) {
  var uintval;
  if (ofs == null) {
    ofs = 0;
  }
  uintval = UINT64(val);
  buf.writeUInt16LE(uintval._a00, ofs + 0);
  buf.writeUInt16LE(uintval._a16, ofs + 2);
  buf.writeUInt16LE(uintval._a32, ofs + 4);
  buf.writeUInt16LE(uintval._a48, ofs + 6);
  return buf;
};

readUINT64 = function(buf, ofs) {
  var hi, lo, val;
  if (ofs == null) {
    ofs = 0;
  }
  lo = buf.readUInt32LE(ofs + 0);
  hi = buf.readUInt32LE(ofs + 4);
  val = UINT64(lo, hi).toNumber();
  return val;
};

readHeader = function(archive, fd) {
  var checksumOfs, checksumSize, err, headerBuf, headerOfs, headerSize, headerSizeBuf, headerSizeOfs, magicBuf, magicLen;
  magicLen = archive.MAGIC.length;
  magicBuf = new Buffer(magicLen);
  if (fs.readSync(fd, magicBuf, 0, magicLen, null) !== magicLen) {
    throw new Error("Unable to open archive: " + archive._archiveName);
  }
  if (magicBuf.toString() !== archive.MAGIC) {
    return readHeaderOld(archive, fd);
  }
  headerSizeOfs = archive._archiveSize - (archive.SIZELENGTH + 16 + archive.SIZELENGTH);
  headerSizeBuf = new Buffer(archive.SIZELENGTH);
  if (fs.readSync(fd, headerSizeBuf, 0, archive.SIZELENGTH, headerSizeOfs) !== archive.SIZELENGTH) {
    throw new Error("Unable to read header size: " + archive._archiveName);
  }
  headerSize = readUINT64(headerSizeBuf);
  headerOfs = archive._archiveSize - headerSize - (archive.SIZELENGTH + 16 + archive.SIZELENGTH);
  headerBuf = new Buffer(headerSize);
  if (fs.readSync(fd, headerBuf, 0, headerSize, headerOfs) !== headerSize) {
    throw new Error("Unable to read header: " + archive._archiveName);
  }
  archive._offset = headerOfs;
  checksumSize = 16;
  checksumOfs = archive._archiveSize - 16 - archive.SIZELENGTH;
  archive._checksum = new Buffer(checksumSize);
  if (fs.readSync(fd, archive._checksum, 0, checksumSize, checksumOfs) !== checksumSize) {
    throw new Error("Unable to read checksum: " + archive._archiveName);
  }
  try {
    archive._header = JSON.parse(headerBuf);
  } catch (_error) {
    err = _error;
    throw new Error("Unable to parse header: " + archive._archiveName);
  }
  archive._headerSize = headerSize;
};

readHeaderOld = function(archive, fd) {
  var actualSize, err, headerBuf, headerStr, size, sizeBuf, sizeBufSize;
  archive._legacyMode = true;
  sizeBufSize = 8;
  sizeBuf = new Buffer(sizeBufSize);
  if (fs.readSync(fd, sizeBuf, 0, sizeBufSize, 0) !== sizeBufSize) {
    throw new Error('Unable to read header size (assumed old format)');
  }
  size = sizeBuf.readUInt32LE(4);
  actualSize = size - 8;
  headerBuf = new Buffer(actualSize);
  if (fs.readSync(fd, headerBuf, 0, actualSize, 16) !== actualSize) {
    throw new Error('Unable to read header (assumed old format)');
  }
  try {
    headerStr = headerBuf.toString().replace(/\0+$/g, '');
    archive._header = JSON.parse(headerStr);
  } catch (_error) {
    err = _error;
    throw new Error('Unable to parse header (assumed old format)');
  }
  archive._headerSize = size;
};

writeHeader = function(archive, out, cb) {
  out.write(archive.MAGIC, cb);
};

writeFooter = function(archive, out, cb) {
  var headerSizeBuf, headerStr;
  if (archive.opts.prettyToc) {
    headerStr = JSON.stringify(archive._header, null, '  ').replace(/\n/g, '\r\n');
    headerStr = "\r\n" + headerStr + "\r\n";
  } else {
    headerStr = JSON.stringify(archive._header);
  }
  archive._headerSize = headerStr.length;
  headerSizeBuf = new Buffer(archive.SIZELENGTH);
  writeUINT64(headerSizeBuf, archive._headerSize);
  out.write(headerStr, function() {
    out.write(headerSizeBuf, function() {
      var archiveFile, md5;
      archiveFile = fs.createReadStream(archive._archiveName);
      md5 = crypto.createHash('md5');
      archiveFile.pipe(md5);
      archiveFile.on('end', function() {
        var archiveSizeBuf;
        archive._checksum = md5.read();
        archive._archiveSize = archive._offset + archive._headerSize + archive.SIZELENGTH + 16 + archive.SIZELENGTH;
        if (archive._archiveSize > MAX_SAFE_INTEGER) {
          return typeof cb === "function" ? cb(new Error("archive size can not be larger than 9PB")) : void 0;
        }
        archiveSizeBuf = new Buffer(archive.SIZELENGTH);
        writeUINT64(archiveSizeBuf, archive._archiveSize);
        out.write(archive._checksum, function() {
          out.write(archiveSizeBuf, cb);
        });
      });
    });
  });
};

openSync = function(archive, archiveName) {
  var err, fd;
  archive.reset();
  archive._archiveName = archiveName;
  try {
    archive._archiveSize = fs.lstatSync(archiveName).size;
    fd = fs.openSync(archiveName, 'r');
    readHeader(archive, fd);
  } catch (_error) {
    err = _error;
    throw err;
  }
  fs.closeSync(fd);
  if ((archive._header.version != null) && archive._header.version > archive.VERSION) {
    throw new Error("Unsupported asar format version: " + archive._header.version + " (max supported: " + archive.VERSION + ")");
  }
  return true;
};

saveArchive = function(archive, archiveName, opts, cb) {
  var appendMode, out, start, writeArchive, writeFile;
  if (typeof opts === 'function') {
    cb = opts;
    opts = {};
  }
  appendMode = archive._archiveName === archiveName;
  archive._archiveName = archiveName;
  mkdirp.sync(path.dirname(archiveName));
  writeFile = function(filename, out, node, cb) {
    var gzip, realSize, src;
    realSize = 0;
    src = fs.createReadStream(filename);
    if (archive.opts.compress && node.size > archive.opts.minSizeToCompress) {
      gzip = zlib.createGzip();
      gzip.on('data', function(chunk) {
        realSize += chunk.length;
      });
      gzip.on('end', function() {
        node.offset = archive._offset;
        node.csize = realSize;
        archive._offset += realSize;
        cb();
      });
      src.pipe(gzip);
      gzip.pipe(out, {
        end: false
      });
    } else {
      src.on('data', function(chunk) {
        realSize += chunk.length;
      });
      src.on('end', function() {
        node.offset = archive._offset;
        archive._offset += realSize;
        cb();
      });
      src.pipe(out, {
        end: false
      });
    }
  };
  writeArchive = function(err, cb) {
    var file, i, q, _i, _len, _ref;
    if (err) {
      return typeof cb === "function" ? cb(err) : void 0;
    }
    q = queue(1);
    _ref = archive._files;
    for (i = _i = 0, _len = _ref.length; _i < _len; i = ++_i) {
      file = _ref[i];
      q.defer(writeFile, file, out, archive._fileNodes[i]);
    }
    q.awaitAll(function(err) {
      if (err) {
        return typeof cb === "function" ? cb(err) : void 0;
      }
      return writeFooter(archive, out, function(err) {
        if (err) {
          return cb(err);
        }
        archive._dirty = false;
        archive._files = [];
        archive._fileNodes = [];
        return cb();
      });
    });
  };
  start = appendMode ? archive._offset : 0;
  if (appendMode) {
    out = fs.createWriteStream(archiveName, {
      flags: 'r+',
      start: start
    });
    writeArchive(null, cb);
  } else {
    out = fs.createWriteStream(archiveName);
    writeHeader(archive, out, function(err) {
      return writeArchive(err, cb);
    });
  }
};

module.exports = {
  loadArchive: function(archive, archiveName) {
    openSync(archive, archiveName);
    return archive;
  },
  saveArchive: function(archive, archiveName, cb) {
    saveArchive(archive, archiveName, cb);
  },
  MAX_SAFE_INTEGER: MAX_SAFE_INTEGER
};
